{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Bayesian hyperparameter tuning of xgBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d157e67d-de73-49e8-8c06-f7912a102ef1",
    "_uuid": "8d84a68ab037f61ce6ce918fcc8e0e120b52ff5c"
   },
   "source": [
    "## XGBoost Parameter Tuning with Scikit-Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "db4170d4-695a-455a-9cb9-a3d6c2a99f21",
    "_uuid": "0861d5e64d73ee7de621d9289095a877f76369e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# SETTINGS - CHANGE THESE TO GET SOMETHING MEANINGFUL\n",
    "ITERATIONS = 10 # 1000\n",
    "TRAINING_SIZE = 100000 # 20000000\n",
    "TEST_SIZE = 25000\n",
    "\n",
    "# Load data\n",
    "X = pd.read_csv(\n",
    "    '../input/train.csv', \n",
    "    skiprows=range(1,184903891-TRAINING_SIZE), \n",
    "    nrows=TRAINING_SIZE,\n",
    "    parse_dates=['click_time']\n",
    ")\n",
    "\n",
    "# Split into X and y\n",
    "y = X['is_attributed']\n",
    "X = X.drop(['click_time','is_attributed', 'attributed_time'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "48b0456d-399c-43b4-b8d3-83c73351e470",
    "_uuid": "93c4122474339eeecc093e9cbcc79d4f4ea2aa0c"
   },
   "source": [
    "To do the bayesian parameter tuning, The [BayesSearchCV](https://scikit-optimize.github.io/#skopt.BayesSearchCV) class of scikit-optimize is used. It works basically as a drop-in replacement for GridSearchCV and RandomSearchCV, but generally we get better results with it. In the following we define the BayesSearchCV object, and write a short convenience function that will be used during optimization to output current status of the tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f8217176-75c2-4819-85c9-95b5afa3a14f",
    "_uuid": "cdc63dc8bb9542c4ad02a308d7c3d984b09dad8e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classifier\n",
    "bayes_cv_tuner = BayesSearchCV(\n",
    "    estimator = xgb.XGBClassifier(\n",
    "        n_jobs = 1,\n",
    "        objective = 'binary:logistic',\n",
    "        eval_metric = 'auc',\n",
    "        silent=1,\n",
    "        tree_method='approx'\n",
    "    ),\n",
    "    search_spaces = {\n",
    "        'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "        'min_child_weight': (0, 10),\n",
    "        'max_depth': (0, 50),\n",
    "        'max_delta_step': (0, 20),\n",
    "        'subsample': (0.01, 1.0, 'uniform'),\n",
    "        'colsample_bytree': (0.01, 1.0, 'uniform'),\n",
    "        'colsample_bylevel': (0.01, 1.0, 'uniform'),\n",
    "        'reg_lambda': (1e-9, 1000, 'log-uniform'),\n",
    "        'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n",
    "        'gamma': (1e-9, 0.5, 'log-uniform'),\n",
    "        'min_child_weight': (0, 5),\n",
    "        'n_estimators': (50, 100),\n",
    "        'scale_pos_weight': (1e-6, 500, 'log-uniform')\n",
    "    },    \n",
    "    scoring = 'roc_auc',\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits=3,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    ),\n",
    "    n_jobs = 3,\n",
    "    n_iter = ITERATIONS,   \n",
    "    verbose = 0,\n",
    "    refit = True,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "def status_print(optim_result):\n",
    "    \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n",
    "    \n",
    "    # Get all the models tested so far in DataFrame format\n",
    "    all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n",
    "    \n",
    "    # Get current parameters and the best parameters    \n",
    "    best_params = pd.Series(bayes_cv_tuner.best_params_)\n",
    "    print('Model #{}\\nBest ROC-AUC: {}\\nBest params: {}\\n'.format(\n",
    "        len(all_models),\n",
    "        np.round(bayes_cv_tuner.best_score_, 4),\n",
    "        bayes_cv_tuner.best_params_\n",
    "    ))\n",
    "    \n",
    "    # Save all model results\n",
    "    clf_name = bayes_cv_tuner.estimator.__class__.__name__\n",
    "    all_models.to_csv(clf_name+\"_cv_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "96cf894e-d153-45f5-829c-58bc427951b5",
    "_uuid": "1a45b9722d17bdf4d97bc75fbf327311b21bd865"
   },
   "source": [
    "Finally, let the parameter tuning run and wait for good results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "997763ce-92ed-463c-9f99-6d2f4af40b62",
    "_uuid": "8f1a1c02de0c17f60ed834267aefdc6c28dd9b78",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "result = bayes_cv_tuner.fit(X.values, y.values, callback=status_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "df2220f9-a7f0-4554-826a-f59073afb9a6",
    "_uuid": "9dfa5be24aec747b6798424b8906e11486b4208c"
   },
   "source": [
    "## LightGBM Parameter Tuning with Scikit-Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3257fa29-0cde-45af-83c3-830730d20df2",
    "_uuid": "784c2278a48af51e01adda8be4471f94d5bf83d8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classifier\n",
    "bayes_cv_tuner = BayesSearchCV(\n",
    "    estimator = lgb.LGBMRegressor(\n",
    "        objective='binary',\n",
    "        metric='auc',\n",
    "        n_jobs=1,\n",
    "        verbose=0\n",
    "    ),\n",
    "    search_spaces = {\n",
    "        'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "        'num_leaves': (1, 100),      \n",
    "        'max_depth': (0, 50),\n",
    "        'min_child_samples': (0, 50),\n",
    "        'max_bin': (100, 1000),\n",
    "        'subsample': (0.01, 1.0, 'uniform'),\n",
    "        'subsample_freq': (0, 10),\n",
    "        'colsample_bytree': (0.01, 1.0, 'uniform'),\n",
    "        'min_child_weight': (0, 10),\n",
    "        'subsample_for_bin': (100000, 500000),\n",
    "        'reg_lambda': (1e-9, 1000, 'log-uniform'),\n",
    "        'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n",
    "        'scale_pos_weight': (1e-6, 500, 'log-uniform'),\n",
    "        'n_estimators': (50, 100),\n",
    "    },    \n",
    "    scoring = 'roc_auc',\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits=3,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    ),\n",
    "    n_jobs = 3,\n",
    "    n_iter = ITERATIONS,   \n",
    "    verbose = 0,\n",
    "    refit = True,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "result = bayes_cv_tuner.fit(X.values, y.values, callback=status_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4fc94139-f487-4c60-9cd6-4c7f19048eea",
    "_uuid": "5b24d7d149418711a5bef90593e2bdf72d29a6a6"
   },
   "source": [
    "## Optimal xgBoost parameters\n",
    "![](http://)After a few days of running for XGBoost, we found the following optimal parameters based on the entire data set after feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{\n",
    "    'colsample_bylevel': 0.1,\n",
    "    'colsample_bytree': 1.0,\n",
    "    'gamma': 5.103973694670875e-08,\n",
    "    'learning_rate': 0.140626707498132,\n",
    "    'max_delta_step': 20,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 4,\n",
    "    'n_estimators': 100,\n",
    "    'reg_alpha': 1e-09,\n",
    "    'reg_lambda': 1000.0,\n",
    "    'scale_pos_weight': 499.99999999999994,\n",
    "    'subsample': 1.0\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
